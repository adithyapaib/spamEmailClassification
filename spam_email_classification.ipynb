{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**SPAM EMAIL CLASSIFICATION**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "dfIDM6he9qSF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mf-tj4SXtbLQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "556bd264-b9de-4531-9a98-167a5fb451fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import os\n",
        "import collections\n",
        "import nltk\n",
        "import urllib.request\n",
        "import os\n",
        "import random\n",
        "from nltk.classify import NaiveBayesClassifier, accuracy\n",
        "from sklearn.utils import shuffle\n",
        "import concurrent.futures\n",
        "import random\n",
        "import re\n",
        "import pickle\n",
        "nltk.download('punkt')\n",
        "# Importing all Required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Rp5aKfCztbLS"
      },
      "outputs": [],
      "source": [
        "# Define few stop words\n",
        "stop_words = {\n",
        "    'ourselves', 'hers', 'between', 'yourself', 'but', 'again','there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they',\n",
        "    'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into','of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as',\n",
        "    'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we','these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more',\n",
        "    'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above','both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any',\n",
        "    'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does','yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can',\n",
        "    'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where','too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't',\n",
        "    'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how','further', 'was', 'here', 'than'}\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5l6AmWRtbK_"
      },
      "source": [
        "We will be using Euron spam dataset for spam email classification problem. The euron datasets ar present at the below location:\n",
        "http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/index.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79S9amo8tbLT"
      },
      "source": [
        "The Euron dataset contains spam (malicious email) and ham (non-malicious email) two folders \n",
        "inside the big folder. Each folder spam and ham then again contain many text files. Let's load the data of the files to a list."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Donwloading the dataset extract and loading it ETL**"
      ],
      "metadata": {
        "id": "Q1DtC8J1HJIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define URLs of all six tar files\n",
        "urls = [\n",
        "    \"http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron1.tar.gz\",\n",
        "    \"http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron2.tar.gz\",\n",
        "    \"http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron3.tar.gz\",\n",
        "    \"http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron4.tar.gz\",\n",
        "    \"http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron5.tar.gz\",\n",
        "    \"http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron6.tar.gz\"\n",
        "]\n",
        "positive_samples = []\n",
        "negative_samples = []\n",
        "\n",
        "# Create a directory to store all the tar files\n",
        "os.makedirs('enron', exist_ok=True)\n",
        "\n",
        "# Download and extract each tar file in parallel\n",
        "def download_extract(url):\n",
        "    filename = os.path.join('enron', url.split(\"/\")[-1])\n",
        "    urllib.request.urlretrieve(url, filename)\n",
        "    os.system(f\"tar -xvf {filename} -C enron\")\n",
        "    os.remove(filename)  # Remove the tar file after extraction\n",
        "    \n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    futures = [executor.submit(download_extract, url) for url in urls]\n",
        "    concurrent.futures.wait(futures)\n",
        "\n",
        "# Define function to load files from a given directory\n",
        "def load_files(directory):\n",
        "    full_data = []\n",
        "    for file_name in os.listdir(directory):\n",
        "        with open(os.path.join(directory, file_name), 'r', encoding='ISO-8859-1') as f:\n",
        "            full_data.append(f.read())\n",
        "    return full_data\n",
        "\n",
        "# Load positive samples (spam) and negative samples (ham) from all six directories\n",
        "for i in range(1, 7):\n",
        "    spam_dir = os.path.join('enron', f'enron{i}', 'spam')\n",
        "    ham_dir = os.path.join('enron', f'enron{i}', 'ham')\n",
        "    positive_samples += load_files(spam_dir)\n",
        "    negative_samples += load_files(ham_dir)\n"
      ],
      "metadata": {
        "id": "54CGszm3vWS2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preporcessing the Data**"
      ],
      "metadata": {
        "id": "Hh6_GOs6HSSI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GH-7Rd8UtbLY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad63d13c-6f8c-4f32-f3fc-16f50b8bf98a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject: market watch undervalued report f\n",
            "april 2004 top pick of the month\n",
            "life energy and technology holdings , inc .\n",
            "( otcbb : leth )\n",
            "leth receives $ 250 , 000 , 000 in financing to fund the manufacture\n",
            "of the environmentally friendly biosphere process system\n",
            "waste - to - energy units in the united states .\n",
            "first unit to roll - out in new orleans in early second quarter .\n",
            "we are expecting earth - shattering upcoming news leading a strong\n",
            "rally in leth for a company that has announced over $ 100 million\n",
            "in sales orders in the past year , and tops that record - setting\n",
            "achievement by acquiring the equivalent of $ 8 . 62 per share in\n",
            "cash for major worldwide expansion .\n",
            "* * our readers grabbed substantial profits for our march pick * *\n",
            "ushg featured at . 75\n",
            "reached 3 . 65 in 8 days !\n",
            "traded as high as 7 . 00 since !\n",
            "the biosphere process system - soaring worldwide demand :\n",
            "leth is utilizing the unique proprietary technology of their\n",
            "biosphere process system to generate revenue from the disposal\n",
            "of a wide variety of waste products at 5 to 7 tons per hour\n",
            "which makes a major impact on the global waste problem . this\n",
            "profitable and environmentally safe process converts into clean ,\n",
            "green electricity such waste materials as municipal solid waste ,\n",
            "agricultural wastes , forestry wastes , medical wastes , industrial\n",
            "wastes , sewage sludge , shale oil , sour natural gas , and the huge\n",
            "market of used tires . leth profits from the sale of electricity\n",
            "created from the waste conversion on a continuous basis by generating\n",
            "5 to 10 mega - watts per hour of electricity which is then sold to\n",
            "replenish the local or national grid .\n",
            "the biosphere process succeeds in filling an urgent worldwide need\n",
            "for cost - effective renewable energy sources and a corresponding\n",
            "universal need to solve critical problems in the disposal of waste .\n",
            "leth has secured worldwide acceptance for a revolutionary product\n",
            "designed to significantly impact the global waste problem while a\n",
            "major push for generating electricity from alternative sources\n",
            "continues to be the hot topic due to shortages and massive power\n",
            "failures .\n",
            "financing of $ 250 million positions leth for astronomical sales :\n",
            "the magnitude of this financing package goes much deeper than the\n",
            "fact that this $ 1 . 50 stock now has accessible capital equivalent to\n",
            "$ 8 . 62 per common share in cash . there are 26 biosphere process\n",
            "systems presently in operation worldwide . the available funding\n",
            "could easily be used to produce 100 additional biospheres . now factor\n",
            "in that the average sale price is $ 7 million per biosphere . we\n",
            "cannot even comprehend what this stock should be trading for with\n",
            "a potential $ 700 , 000 , 000 in future sales with 29 million shares\n",
            "outstanding !\n",
            "leth stock guidance :\n",
            "current price : 1 . 80\n",
            "near - term target : 4 . 80\n",
            "projected high for ' 04 : 12 . 50\n",
            "leth ' s blue chip partner - fortifying the system :\n",
            "leth is an alliance partner with tetra tech , inc . ( nasdaq : ttek , $ 21 )\n",
            "a leader and one of the largest providers in environmental , mechanical ,\n",
            "and electrical management consulting services primarily for the us\n",
            "government with annual sales of $ 800 million . tetra tech will\n",
            "coordinate the securing of necessary permits , installation , and\n",
            "continuous worldwide monitoring of the biosphere process system\n",
            "for leth . tetra tech is now in the process of obtaining department\n",
            "of environmental quality permitting for the biosphere process in\n",
            "the state of louisiana . this is a monumental event for leth which\n",
            "opens the floodgates for major project revenues in louisiana while\n",
            "having a parallel effect on leth stock in the form of a huge\n",
            "near - term announcement .\n",
            "political power fosters rapid global expansion :\n",
            "leth has captured the profit - making attention of both us and\n",
            "international investors by embracing a major foothold on the global\n",
            "waste problem as well as the urgent need to generate electricity\n",
            "from alternative sources . this has been accomplished by successfully\n",
            "creating major inroads to all corners of the globe through the\n",
            "political contacts at the highest level from dr . albert reynolds ,\n",
            "chairman of leth , who is also the former prime minister of ireland .\n",
            "dr . reynolds international stature has been instrumental in guiding\n",
            "leth into a position of worldwide dominance in an industry with\n",
            "such high global demand that it is impossible to assign a value\n",
            "to the size of the market .\n",
            "uncommon value for a company of this caliber :\n",
            "we are witnessing a breakout year in the making judging by the\n",
            "frequency of recently announced sales contracts for the biosphere ,\n",
            "the impressive backlog of over $ 100 million in sales orders , and\n",
            "the company ' s very solid financial position . we view this perfectly\n",
            "timed convergence of events as the catalyst for additional\n",
            "contracts that will perpetuate the shattering of the company ' s\n",
            "own sales records . as our top stock pick for april , we anticipate\n",
            "the continuation of strong positive developments that will\n",
            "ignite leth shares which carry our highest rating for short - term\n",
            "trading profits followed by robust long - term capital gains .\n",
            "top pick of the month cautions that small and micro - cap stocks are\n",
            "high - risk investments and that some or all investment dollars can\n",
            "be lost . we suggest you consult a professional investment advisor\n",
            "before purchasing any stock . all opinions expressed on the\n",
            "featured company are the opinions of top pick of the month .\n",
            "top pick of the month recommends you use the information found\n",
            "here as an initial starting point for conducting your own\n",
            "research and your own due diligence on the featured company in\n",
            "order to determine your own personal opinion of the company\n",
            "before investing . top pick of the month is not an investment\n",
            "advisor , financial planning service or a stock brokerage firm\n",
            "and in accordance with such is not offering investment advice\n",
            "or promoting any investment strategies . top pick of the month\n",
            "is not offering securities for sale or solicitation of any offer\n",
            "to buy or sell securities . top pick of the month has received\n",
            "twenty eight thousand dollars from an unaffiliated third party\n",
            "for the preparation of this company profile . since we have\n",
            "received compensation there is an inherent conflict of interest\n",
            "in our statements and opinions . readers of this publication are\n",
            "cautioned not to place undue reliance on forward looking\n",
            "statements , which are based on certain assumptions and\n",
            "expectations involving various risks and uncertainties , that\n",
            "could cause results to differ materially from those set forth\n",
            "in the forward looking statements .\n",
            "hfu ke hg y dc lfl fuz\n",
            "wwolytile\n",
            "zoxqmrvko kgst glkdss\n",
            "tqagfgexdcnv eka\n",
            "\n",
            "33716 emails processed\n"
          ]
        }
      ],
      "source": [
        "# Preprocessing the data includes lemmatization, tokenization and stop word removal\n",
        "nltk.download('wordnet')\n",
        "def preprocess_sentence(sentence):\n",
        "    lemmatizer = nltk.WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(w.lower()) for w in nltk.word_tokenize(sentence) if w.lower() not in stop_words]\n",
        "    word_counts = collections.Counter(tokens)\n",
        "    uncommon_words = word_counts.most_common()[:-10:-1]\n",
        "    tokens = [w for w in tokens if w not in uncommon_words]\n",
        "    return tokens\n",
        "# Let us have a look at an email\n",
        "for email in positive_samples[:1]:\n",
        "  print(email)\n",
        "\n",
        "# preprocess sentences \n",
        "positive_samples = [preprocess_sentence(email) for email in positive_samples]\n",
        "negative_samples = [preprocess_sentence(email) for email in negative_samples]\n",
        "\n",
        "# label samples\n",
        "positive_samples = [(email, 1) for email in positive_samples]\n",
        "negative_samples = [(email, 0) for email in negative_samples]\n",
        "all_samples = positive_samples + negative_samples\n",
        "random.shuffle(all_samples)\n",
        "print(f\"{len(all_samples)} emails processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ChDCZZ5RtbLb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Feature extraction\n",
        "def feature_extraction(tokens):\n",
        "    # Each word will be a feature and feature value| will be word count\n",
        "    return dict(collections.Counter(tokens))\n",
        "    # features = [(feature_extraction(corpus), label) for corpus, label in all_samples]\n",
        "features = [(feature_extraction(corpus), label)\n",
        "              for corpus, label in all_samples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-TQSIGTtbLc",
        "outputId": "11e827fe-314c-433f-ea8a-df03d8b6b508",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[({'subject': 1,\n",
              "   ':': 1,\n",
              "   'software': 1,\n",
              "   'incredibly': 1,\n",
              "   'low': 2,\n",
              "   'price': 1,\n",
              "   '(': 1,\n",
              "   '83': 1,\n",
              "   '%': 1,\n",
              "   'lower': 1,\n",
              "   ')': 1,\n",
              "   '.': 14,\n",
              "   'unconscionable': 1,\n",
              "   'botching': 1,\n",
              "   'gentle': 1,\n",
              "   'always': 1,\n",
              "   ',': 9,\n",
              "   'vary': 1,\n",
              "   'river': 1,\n",
              "   'joy': 1,\n",
              "   'hour': 1,\n",
              "   'phrase': 1,\n",
              "   'early': 2,\n",
              "   'hair': 1,\n",
              "   'grass': 1,\n",
              "   'century': 1,\n",
              "   'numeral': 1,\n",
              "   'verb': 1,\n",
              "   'wire': 1,\n",
              "   'behind': 1,\n",
              "   'thousand': 1,\n",
              "   'face': 1,\n",
              "   'common': 1,\n",
              "   'fine': 1,\n",
              "   'snow': 1,\n",
              "   'long': 1,\n",
              "   'lie': 1,\n",
              "   'case': 1,\n",
              "   'wash': 1,\n",
              "   'job': 1,\n",
              "   'complete': 1,\n",
              "   'make': 1,\n",
              "   'cover': 1,\n",
              "   'bank': 1,\n",
              "   'hit': 1,\n",
              "   'soldier': 1,\n",
              "   'serve': 1,\n",
              "   'well': 1,\n",
              "   'force': 1,\n",
              "   'fly': 1,\n",
              "   'every': 1,\n",
              "   'round': 1,\n",
              "   'laugh': 1,\n",
              "   'smell': 1,\n",
              "   'mind': 1,\n",
              "   'rather': 1,\n",
              "   'strong': 1,\n",
              "   'village': 1,\n",
              "   'lay': 1,\n",
              "   'fraction': 1,\n",
              "   'north': 1,\n",
              "   'trade': 1,\n",
              "   'sit': 1,\n",
              "   'particular': 1},\n",
              "  1),\n",
              " ({'subject': 3,\n",
              "   ':': 13,\n",
              "   'june': 3,\n",
              "   '2000': 7,\n",
              "   'co': 4,\n",
              "   '-': 104,\n",
              "   'owner': 4,\n",
              "   'volume': 4,\n",
              "   'forwarded': 2,\n",
              "   'ami': 2,\n",
              "   'chokshi': 2,\n",
              "   '/': 12,\n",
              "   'corp': 2,\n",
              "   'enron': 2,\n",
              "   '05': 9,\n",
              "   '25': 4,\n",
              "   '03': 3,\n",
              "   'pm': 4,\n",
              "   \"''\": 2,\n",
              "   'steve': 2,\n",
              "   'holmes': 1,\n",
              "   '``': 2,\n",
              "   '40': 1,\n",
              "   'cc': 2,\n",
              "   'please': 1,\n",
              "   'see': 1,\n",
              "   'attached': 1,\n",
              "   'list': 1,\n",
              "   '.': 3,\n",
              "   '0600': 1,\n",
              "   'enronl': 1,\n",
              "   'xl': 1,\n",
              "   'jacqueline': 1,\n",
              "   'blanchard': 1,\n",
              "   '07': 1,\n",
              "   'quarantine': 1,\n",
              "   'bay': 1,\n",
              "   'text': 1,\n",
              "   'htm': 1},\n",
              "  0),\n",
              " ({'subject': 1,\n",
              "   ':': 5,\n",
              "   're': 1,\n",
              "   '[': 1,\n",
              "   'appendage': 1,\n",
              "   ']': 1,\n",
              "   '83': 1,\n",
              "   '%': 1,\n",
              "   '-': 7,\n",
              "   'vicodin': 1,\n",
              "   '.': 4,\n",
              "   'carelessness': 1,\n",
              "   'reach': 1,\n",
              "   'inheritance': 1,\n",
              "   'remarkableness': 1,\n",
              "   'correction': 1,\n",
              "   'derek': 1,\n",
              "   'equalizer': 1,\n",
              "   'ellipsoid': 1,\n",
              "   'canadianization': 1,\n",
              "   'zonally': 1,\n",
              "   'styler': 1,\n",
              "   'effectively': 1,\n",
              "   'seven': 1,\n",
              "   'hobby': 1,\n",
              "   'catered': 1,\n",
              "   'pressing': 1,\n",
              "   'fine': 1,\n",
              "   'daffodil': 1,\n",
              "   'dormant': 1,\n",
              "   'maddest': 1,\n",
              "   'anatomic': 1,\n",
              "   'trudge': 1,\n",
              "   'appertains': 1,\n",
              "   'retort': 1,\n",
              "   'recessed': 1,\n",
              "   'gibby': 1,\n",
              "   'interested': 1,\n",
              "   'superlative': 1,\n",
              "   'sour': 1,\n",
              "   'blackwells': 1,\n",
              "   'blaming': 1,\n",
              "   'mender': 1,\n",
              "   'hater': 1,\n",
              "   'rainbow': 1,\n",
              "   'seaward': 1,\n",
              "   'dutch': 1,\n",
              "   'amused': 1,\n",
              "   'bloody': 1,\n",
              "   'inspects': 1,\n",
              "   'preposition': 1,\n",
              "   'phone': 1,\n",
              "   '831': 1,\n",
              "   '863': 1,\n",
              "   '5130': 1,\n",
              "   'mobile': 1,\n",
              "   '155': 1,\n",
              "   '231': 1,\n",
              "   '1088': 1,\n",
              "   'email': 1,\n",
              "   'aida': 1,\n",
              "   'cooper': 1,\n",
              "   '2001': 1,\n",
              "   '@': 1,\n",
              "   'neo': 1,\n",
              "   'rr': 1,\n",
              "   'com': 1},\n",
              "  1)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "features[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and Testing The Model**"
      ],
      "metadata": {
        "id": "0yKkMXcvEwfz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6ZVA2-pEtbLd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4271ddd-449b-4114-8653-91b1bdfa1bb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training complete. Accuracy on training set: 0.9952968094572264\n",
            "Accuracy on test set: 0.9887296094908552\n"
          ]
        }
      ],
      "source": [
        "# train test split\n",
        "def train_test_split(dataset, train_size=0.8):\n",
        "    num_train_samples = int(len(dataset) * train_size)\n",
        "    return dataset[:num_train_samples], dataset[num_train_samples:]\n",
        "\n",
        "training_set, test_set = train_test_split(features, train_size=0.7)\n",
        "model = nltk.classify.NaiveBayesClassifier.train(training_set)\n",
        "training_error = nltk.classify.accuracy(model, training_set)\n",
        "print(f'Model training complete. Accuracy on training set: {training_error}')\n",
        "testing_error = nltk.classify.accuracy(model, test_set)\n",
        "print(f'Accuracy on test set: {testing_error}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Custom Testing Of the Project**\n",
        "\n"
      ],
      "metadata": {
        "id": "CunpECkR8qCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_spam(input_data):\n",
        "    preprocessed_email = preprocess_sentence(input_data)\n",
        "    features = feature_extraction(preprocessed_email)\n",
        "    prediction = model.classify(features)\n",
        "    if prediction == 1:\n",
        "        print(\"SPAM EMAIL\")\n",
        "    else:\n",
        "        print(\"HAM EMAIL\")\n"
      ],
      "metadata": {
        "id": "iCXY_SqMJMTP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "a = \"\"\"\n",
        "\t\n",
        "Hello, user-1123454\n",
        "\n",
        "\n",
        "\n",
        "364 days ago, you registered on our platform for automatic cloud Bitcoin mining (collection) by living your device to our platform by IP address.\n",
        "\n",
        "You were not active in your personal account, but the collection of cryptocurrency accrued automatically from your device.\n",
        "\n",
        "\n",
        "\n",
        "You balance $24356.45\n",
        "\n",
        "\n",
        "\n",
        "Your account will be deleted after 24 hour\n",
        "\n",
        "Best regards support team Bit_Bonus\n",
        "\"\"\"\n",
        "\n",
        "custom_email2 = \"\"\"\n",
        "Greetings, I am Mrs.Fatim Adama aging widow of 62 years old suffering\n",
        "from long time illness.I have some funds I inherited from my late\n",
        "husband, the sum of ($18,500,000.00 Million Dollars) and I needed a\n",
        "very honest and God fearing who can withdraw this money this funds use\n",
        "it for Charity works.I found your email address from the internet after\n",
        "honest prayers to the LORD to bring me a helper and i decided to\n",
        "contact you if you may be willing and interested to handle these trust\n",
        "funds in good faith. More detail will be giving after receiving your\n",
        "reply.Please kindly respond quickly for further details through my\n",
        "private e_mail address:(mrs.fatimadama@yahoo.com) Warmest Regards,\n",
        "Mrs.Fatim Adama.\n",
        "\"\"\"\n",
        "\n",
        "test = \"\"\"\n",
        "Dear students,\n",
        "Kindly find the updated schedule for the internship presentation. Submit the draft copy of the report on or before 10-4-2023.\n",
        "\n",
        "Note: Change in schedule is not permitted.\n",
        " \n",
        "Regards,\n",
        "\n",
        "Pavithra D S\n",
        "Assistant Professor\n",
        "Dept of CSE\n",
        "Canara Engineering College\n",
        "Benjanapadavu\n",
        "\"\"\"\n",
        "\n",
        "check_spam(a)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAlBl9fTNyS7",
        "outputId": "af042ea0-e6ab-4057-a683-16bd994b92d3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi\n",
            "SPAM EMAIL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PutY4CPkDXXG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}