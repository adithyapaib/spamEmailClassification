{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**SPAM EMAIL CLASSIFICATION**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "dfIDM6he9qSF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mf-tj4SXtbLQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b8ceff4-82cb-48d7-cd49-3f744e62f838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import os\n",
        "import collections\n",
        "import nltk\n",
        "import urllib.request\n",
        "import os\n",
        "import random\n",
        "from nltk.classify import NaiveBayesClassifier, accuracy\n",
        "from sklearn.utils import shuffle\n",
        "import concurrent.futures\n",
        "import random\n",
        "import re\n",
        "import pickle\n",
        "nltk.download('punkt')\n",
        "# Importing all Required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rp5aKfCztbLS"
      },
      "outputs": [],
      "source": [
        "# Define few stop words\n",
        "stop_words = {\n",
        "    'ourselves', 'hers', 'between', 'yourself', 'but', 'again','there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they',\n",
        "    'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into','of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as',\n",
        "    'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we','these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more',\n",
        "    'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above','both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any',\n",
        "    'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does','yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can',\n",
        "    'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where','too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't',\n",
        "    'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how','further', 'was', 'here', 'than'}\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5l6AmWRtbK_"
      },
      "source": [
        "We will be using Euron spam dataset for spam email classification problem. The euron datasets ar present at the below location:\n",
        "http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/index.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79S9amo8tbLT"
      },
      "source": [
        "The Euron dataset contains spam (malicious email) and ham (non-malicious email) two folders \n",
        "inside the big folder. Each folder spam and ham then again contain many text files. Let's load the data of the files to a list."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Donwloading the dataset extract and loading it ETL**"
      ],
      "metadata": {
        "id": "Q1DtC8J1HJIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define URLs of all six tar files\n",
        "urls = [\n",
        "    \"http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron1.tar.gz\",\n",
        "    \"http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron2.tar.gz\",\n",
        "    \"http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron3.tar.gz\",\n",
        "    \"http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron4.tar.gz\",\n",
        "    \"http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron5.tar.gz\",\n",
        "    \"http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron6.tar.gz\"\n",
        "]\n",
        "positive_samples = []\n",
        "negative_samples = []\n",
        "\n",
        "# Create a directory to store all the tar files\n",
        "os.makedirs('enron', exist_ok=True)\n",
        "\n",
        "# Download and extract each tar file in parallel\n",
        "def download_extract(url):\n",
        "    filename = os.path.join('enron', url.split(\"/\")[-1])\n",
        "    urllib.request.urlretrieve(url, filename)\n",
        "    os.system(f\"tar -xvf {filename} -C enron\")\n",
        "    os.remove(filename)  # Remove the tar file after extraction\n",
        "    \n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    futures = [executor.submit(download_extract, url) for url in urls]\n",
        "    concurrent.futures.wait(futures)\n",
        "\n",
        "# Define function to load files from a given directory\n",
        "def load_files(directory):\n",
        "    full_data = []\n",
        "    for file_name in os.listdir(directory):\n",
        "        with open(os.path.join(directory, file_name), 'r', encoding='ISO-8859-1') as f:\n",
        "            full_data.append(f.read())\n",
        "    return full_data\n",
        "\n",
        "# Load positive samples (spam) and negative samples (ham) from all six directories\n",
        "for i in range(1, 7):\n",
        "    spam_dir = os.path.join('enron', f'enron{i}', 'spam')\n",
        "    ham_dir = os.path.join('enron', f'enron{i}', 'ham')\n",
        "    positive_samples += load_files(spam_dir)\n",
        "    negative_samples += load_files(ham_dir)\n"
      ],
      "metadata": {
        "id": "54CGszm3vWS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preporcessing the Data**"
      ],
      "metadata": {
        "id": "Hh6_GOs6HSSI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GH-7Rd8UtbLY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dcf6feb-8707-4cc7-9fe9-c501e3bea8cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject: new schedule\n",
            "cry born , cow mount record middle . govern , nor song problem\n",
            "world . go invent , strong see many no , twenty . tube past are give\n",
            "eight first clock . able top shout fun thus wire . east , most we .\n",
            "page wish fast . by move winter . hurry area small each . other ,\n",
            "there , then . your second , farm this name differ when . red am\n",
            "school with reason . square than burn .\n",
            "- -\n",
            "phone : 359 - 716 - 9493\n",
            "mobile : 608 - 244 - 1895\n",
            "email : zoomssparking @ speedy . net . pe\n",
            "\n",
            "33716 emails processed\n"
          ]
        }
      ],
      "source": [
        "# Preprocessing the data includes lemmatization, tokenization and stop word removal\n",
        "nltk.download('wordnet')\n",
        "def preprocess_sentence(sentence):\n",
        "    lemmatizer = nltk.WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(w.lower()) for w in nltk.word_tokenize(sentence) if w.lower() not in stop_words]\n",
        "    word_counts = collections.Counter(tokens)\n",
        "    uncommon_words = word_counts.most_common()[:-10:-1]\n",
        "    tokens = [w for w in tokens if w not in uncommon_words]\n",
        "    return tokens\n",
        "# Let us have a look at an email\n",
        "for email in positive_samples[:1]:\n",
        "  print(email)\n",
        "\n",
        "# preprocess sentences \n",
        "positive_samples = [preprocess_sentence(email) for email in positive_samples]\n",
        "negative_samples = [preprocess_sentence(email) for email in negative_samples]\n",
        "\n",
        "# label samples\n",
        "positive_samples = [(email, 1) for email in positive_samples]\n",
        "negative_samples = [(email, 0) for email in negative_samples]\n",
        "all_samples = positive_samples + negative_samples\n",
        "random.shuffle(all_samples)\n",
        "print(f\"{len(all_samples)} emails processed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChDCZZ5RtbLb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Feature extraction\n",
        "def feature_extraction(tokens):\n",
        "    # Each word will be a feature and feature value| will be word count\n",
        "    return dict(collections.Counter(tokens))\n",
        "    # features = [(feature_extraction(corpus), label) for corpus, label in all_samples]\n",
        "features = [(feature_extraction(corpus), label)\n",
        "              for corpus, label in all_samples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-TQSIGTtbLc",
        "outputId": "11e827fe-314c-433f-ea8a-df03d8b6b508",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[({'subject': 1,\n",
              "   ':': 1,\n",
              "   'software': 1,\n",
              "   'incredibly': 1,\n",
              "   'low': 2,\n",
              "   'price': 1,\n",
              "   '(': 1,\n",
              "   '83': 1,\n",
              "   '%': 1,\n",
              "   'lower': 1,\n",
              "   ')': 1,\n",
              "   '.': 14,\n",
              "   'unconscionable': 1,\n",
              "   'botching': 1,\n",
              "   'gentle': 1,\n",
              "   'always': 1,\n",
              "   ',': 9,\n",
              "   'vary': 1,\n",
              "   'river': 1,\n",
              "   'joy': 1,\n",
              "   'hour': 1,\n",
              "   'phrase': 1,\n",
              "   'early': 2,\n",
              "   'hair': 1,\n",
              "   'grass': 1,\n",
              "   'century': 1,\n",
              "   'numeral': 1,\n",
              "   'verb': 1,\n",
              "   'wire': 1,\n",
              "   'behind': 1,\n",
              "   'thousand': 1,\n",
              "   'face': 1,\n",
              "   'common': 1,\n",
              "   'fine': 1,\n",
              "   'snow': 1,\n",
              "   'long': 1,\n",
              "   'lie': 1,\n",
              "   'case': 1,\n",
              "   'wash': 1,\n",
              "   'job': 1,\n",
              "   'complete': 1,\n",
              "   'make': 1,\n",
              "   'cover': 1,\n",
              "   'bank': 1,\n",
              "   'hit': 1,\n",
              "   'soldier': 1,\n",
              "   'serve': 1,\n",
              "   'well': 1,\n",
              "   'force': 1,\n",
              "   'fly': 1,\n",
              "   'every': 1,\n",
              "   'round': 1,\n",
              "   'laugh': 1,\n",
              "   'smell': 1,\n",
              "   'mind': 1,\n",
              "   'rather': 1,\n",
              "   'strong': 1,\n",
              "   'village': 1,\n",
              "   'lay': 1,\n",
              "   'fraction': 1,\n",
              "   'north': 1,\n",
              "   'trade': 1,\n",
              "   'sit': 1,\n",
              "   'particular': 1},\n",
              "  1),\n",
              " ({'subject': 3,\n",
              "   ':': 13,\n",
              "   'june': 3,\n",
              "   '2000': 7,\n",
              "   'co': 4,\n",
              "   '-': 104,\n",
              "   'owner': 4,\n",
              "   'volume': 4,\n",
              "   'forwarded': 2,\n",
              "   'ami': 2,\n",
              "   'chokshi': 2,\n",
              "   '/': 12,\n",
              "   'corp': 2,\n",
              "   'enron': 2,\n",
              "   '05': 9,\n",
              "   '25': 4,\n",
              "   '03': 3,\n",
              "   'pm': 4,\n",
              "   \"''\": 2,\n",
              "   'steve': 2,\n",
              "   'holmes': 1,\n",
              "   '``': 2,\n",
              "   '40': 1,\n",
              "   'cc': 2,\n",
              "   'please': 1,\n",
              "   'see': 1,\n",
              "   'attached': 1,\n",
              "   'list': 1,\n",
              "   '.': 3,\n",
              "   '0600': 1,\n",
              "   'enronl': 1,\n",
              "   'xl': 1,\n",
              "   'jacqueline': 1,\n",
              "   'blanchard': 1,\n",
              "   '07': 1,\n",
              "   'quarantine': 1,\n",
              "   'bay': 1,\n",
              "   'text': 1,\n",
              "   'htm': 1},\n",
              "  0),\n",
              " ({'subject': 1,\n",
              "   ':': 5,\n",
              "   're': 1,\n",
              "   '[': 1,\n",
              "   'appendage': 1,\n",
              "   ']': 1,\n",
              "   '83': 1,\n",
              "   '%': 1,\n",
              "   '-': 7,\n",
              "   'vicodin': 1,\n",
              "   '.': 4,\n",
              "   'carelessness': 1,\n",
              "   'reach': 1,\n",
              "   'inheritance': 1,\n",
              "   'remarkableness': 1,\n",
              "   'correction': 1,\n",
              "   'derek': 1,\n",
              "   'equalizer': 1,\n",
              "   'ellipsoid': 1,\n",
              "   'canadianization': 1,\n",
              "   'zonally': 1,\n",
              "   'styler': 1,\n",
              "   'effectively': 1,\n",
              "   'seven': 1,\n",
              "   'hobby': 1,\n",
              "   'catered': 1,\n",
              "   'pressing': 1,\n",
              "   'fine': 1,\n",
              "   'daffodil': 1,\n",
              "   'dormant': 1,\n",
              "   'maddest': 1,\n",
              "   'anatomic': 1,\n",
              "   'trudge': 1,\n",
              "   'appertains': 1,\n",
              "   'retort': 1,\n",
              "   'recessed': 1,\n",
              "   'gibby': 1,\n",
              "   'interested': 1,\n",
              "   'superlative': 1,\n",
              "   'sour': 1,\n",
              "   'blackwells': 1,\n",
              "   'blaming': 1,\n",
              "   'mender': 1,\n",
              "   'hater': 1,\n",
              "   'rainbow': 1,\n",
              "   'seaward': 1,\n",
              "   'dutch': 1,\n",
              "   'amused': 1,\n",
              "   'bloody': 1,\n",
              "   'inspects': 1,\n",
              "   'preposition': 1,\n",
              "   'phone': 1,\n",
              "   '831': 1,\n",
              "   '863': 1,\n",
              "   '5130': 1,\n",
              "   'mobile': 1,\n",
              "   '155': 1,\n",
              "   '231': 1,\n",
              "   '1088': 1,\n",
              "   'email': 1,\n",
              "   'aida': 1,\n",
              "   'cooper': 1,\n",
              "   '2001': 1,\n",
              "   '@': 1,\n",
              "   'neo': 1,\n",
              "   'rr': 1,\n",
              "   'com': 1},\n",
              "  1)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "features[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and Testing The Model**"
      ],
      "metadata": {
        "id": "0yKkMXcvEwfz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZVA2-pEtbLd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4271ddd-449b-4114-8653-91b1bdfa1bb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training complete. Accuracy on training set: 0.9952968094572264\n",
            "Accuracy on test set: 0.9887296094908552\n"
          ]
        }
      ],
      "source": [
        "# train test split\n",
        "def train_test_split(dataset, train_size=0.8):\n",
        "    num_train_samples = int(len(dataset) * train_size)\n",
        "    return dataset[:num_train_samples], dataset[num_train_samples:]\n",
        "\n",
        "training_set, test_set = train_test_split(features, train_size=0.7)\n",
        "model = nltk.classify.NaiveBayesClassifier.train(training_set)\n",
        "training_error = nltk.classify.accuracy(model, training_set)\n",
        "print(f'Model training complete. Accuracy on training set: {training_error}')\n",
        "testing_error = nltk.classify.accuracy(model, test_set)\n",
        "print(f'Accuracy on test set: {testing_error}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Custom Testing Of the Project**\n",
        "\n"
      ],
      "metadata": {
        "id": "CunpECkR8qCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_spam(input_data):\n",
        "    preprocessed_email = preprocess_sentence(input_data)\n",
        "    features = feature_extraction(preprocessed_email)\n",
        "    prediction = model.classify(features)\n",
        "    if prediction == 1:\n",
        "        print(\"SPAM EMAIL\")\n",
        "    else:\n",
        "        print(\"HAM EMAIL\")\n"
      ],
      "metadata": {
        "id": "iCXY_SqMJMTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "a = \"\"\"\n",
        "\t\n",
        "Hello, user-1123454\n",
        "\n",
        "\n",
        "\n",
        "364 days ago, you registered on our platform for automatic cloud Bitcoin mining (collection) by living your device to our platform by IP address.\n",
        "\n",
        "You were not active in your personal account, but the collection of cryptocurrency accrued automatically from your device.\n",
        "\n",
        "\n",
        "\n",
        "You balance $24356.45\n",
        "\n",
        "\n",
        "\n",
        "Your account will be deleted after 24 hour\n",
        "\n",
        "Best regards support team Bit_Bonus\n",
        "\"\"\"\n",
        "\n",
        "custom_email2 = \"\"\"\n",
        "Greetings, I am Mrs.Fatim Adama aging widow of 62 years old suffering\n",
        "from long time illness.I have some funds I inherited from my late\n",
        "husband, the sum of ($18,500,000.00 Million Dollars) and I needed a\n",
        "very honest and God fearing who can withdraw this money this funds use\n",
        "it for Charity works.I found your email address from the internet after\n",
        "honest prayers to the LORD to bring me a helper and i decided to\n",
        "contact you if you may be willing and interested to handle these trust\n",
        "funds in good faith. More detail will be giving after receiving your\n",
        "reply.Please kindly respond quickly for further details through my\n",
        "private e_mail address:(mrs.fatimadama@yahoo.com) Warmest Regards,\n",
        "Mrs.Fatim Adama.\n",
        "\"\"\"\n",
        "\n",
        "test = \"\"\"\n",
        "Dear students,\n",
        "Kindly find the updated schedule for the internship presentation. Submit the draft copy of the report on or before 10-4-2023.\n",
        "\n",
        "Note: Change in schedule is not permitted.\n",
        " \n",
        "Regards,\n",
        "\n",
        "Pavithra D S\n",
        "Assistant Professor\n",
        "Dept of CSE\n",
        "Canara Engineering College\n",
        "Benjanapadavu\n",
        "\"\"\"\n",
        "\n",
        "check_spam(a)\n"
      ],
      "metadata": {
        "id": "LAlBl9fTNyS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extras**"
      ],
      "metadata": {
        "id": "agTVSZuJGMwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Export Model\n",
        "import pickle\n",
        "\n",
        "# Create a file object to write the model to.\n",
        "with open(\"model.pkl\", \"wb\") as f:\n",
        "    # Use the pickle.dump() function to serialize the model to the file object.\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "# Close the file object.\n",
        "f.close()"
      ],
      "metadata": {
        "id": "IgyDTh9wMhaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Run with Pre Exported Dataset**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XYGT-ARbFvku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, nltk, collections, pickle\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "stop_words = {'ourselves', 'hers', 'between', 'yourself', 'but', 'again','there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they','own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into','of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as','from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we','these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more','himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above','both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any','before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does','yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can','did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where','too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't','being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how','further', 'was', 'here', 'than'}\n",
        "open(\"model.pkl\", \"wb\").write(requests.get(\"https://github.com/adithyapaib/spamEmailClassification/raw/main/model.pkl\").content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmFmlHjcIWpF",
        "outputId": "241cf29c-451d-458a-dd09-f2a49b2d7e40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18693944"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    lemmatizer = nltk.WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(w.lower()) for w in nltk.word_tokenize(sentence) if w.lower() not in stop_words]\n",
        "    return [w for w in tokens if w not in list(collections.Counter(tokens).most_common()[:-10:-1])]\n",
        "\n",
        "\n",
        "def check_spam(input_data):\n",
        "    if pickle.load(open('model.pkl', 'rb')).classify(dict(collections.Counter(preprocess_sentence(input_data)))) == 1:\n",
        "        return print(\"\\033[31m*******************\\n*   SPAM EMAIL !  *\\n*******************\\033[0m\")\n",
        "    print(\"\\033[32m*******************\\n*   HAM EMAIL !   *\\n*******************\\033[0m\")\n",
        "\n",
        "\n",
        "test = \"\"\"\n",
        "Dear students,\n",
        "Kindly find the updated schedule for the internship presentation. Submit the draft copy of the report on or before 10-4-2023.\n",
        "\n",
        "Note: Change in schedule is not permitted.\n",
        " \n",
        "Regards,\n",
        "\n",
        "Pavithra D S\n",
        "Assistant Professor\n",
        "Dept of CSE\n",
        "Canara Engineering College\n",
        "Benjanapadavu\n",
        "\"\"\"\n",
        "\n",
        "spam =\"\"\"\n",
        "\n",
        "Hello, user-1123454\n",
        "\n",
        "\n",
        "\n",
        "364 days ago, you registered on our platform for automatic cloud Bitcoin mining (collection) by living your device to our platform by IP address.\n",
        "\n",
        "You were not active in your personal account, but the collection of cryptocurrency accrued automatically from your device.\n",
        "\n",
        "\n",
        "\n",
        "You balance $24356.45\n",
        "\n",
        "\n",
        "\n",
        "Your account will be deleted after 24 hour\n",
        "\n",
        "Best regards support team Bit_Bonus\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "check_spam(spam)\n"
      ],
      "metadata": {
        "id": "1ZcUaQ_2GMGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44460af2-cbdc-47ce-a19e-15c9425fe3a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31m*******************\n",
            "*   SPAM EMAIL !  *\n",
            "*******************\u001b[0m\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}